{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \n# visualisation libraries\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nfrom numpy.random import RandomState\n\n\n#word cloud\n%matplotlib inline\nimport wordcloud\n\nfrom wordcloud import WordCloud, STOPWORDS\n%matplotlib inline\nsns.set()\n\n# visualisation libraries\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nfrom plotly.offline import init_notebook_mode, plot, iplot\nimport plotly.graph_objs as go\ninit_notebook_mode(connected=True)\n\n\n# ML Models\nfrom surprise import Reader\nfrom surprise import Dataset\nfrom surprise.model_selection import cross_validate\nfrom surprise import NormalPredictor\nfrom surprise import KNNBasic\nfrom surprise import KNNWithMeans\nfrom surprise import KNNWithZScore\nfrom surprise import KNNBaseline\nfrom surprise import SVD\nfrom surprise import BaselineOnly\nfrom surprise import SVDpp\nfrom surprise import NMF\nfrom surprise import SlopeOne\nfrom surprise import CoClustering\nfrom surprise.accuracy import rmse\nfrom surprise import accuracy\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n# ML Pre processing\nfrom surprise.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\n# Hyperparameter tuning\nfrom surprise.model_selection import GridSearchCV\n\n# High performance hyperparameter tuning\n#from tune_sklearn import TuneSearchCV\n#import warnings\n#warnings.filterwarnings(\"ignore\")\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-01T18:20:02.180469Z","iopub.execute_input":"2022-05-01T18:20:02.180739Z","iopub.status.idle":"2022-05-01T18:20:03.271028Z","shell.execute_reply.started":"2022-05-01T18:20:02.180709Z","shell.execute_reply":"2022-05-01T18:20:03.270305Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport scipy as sp\nfrom wordcloud import WordCloud, STOPWORDS\nimport os\n#for dirname, _, filenames in os.walk('/kaggle/input'):\n#    for filename in filenames:\n#        print(os.path.join(dirname, filename))\n        \nimport surprise\nfrom surprise import Reader\nfrom surprise import Dataset\nfrom surprise.model_selection import train_test_split\nimport time\nfrom surprise import SVD\nfrom surprise import accuracy\nimport re\nimport plotly.express as px\nimport scipy as sp\nfrom wordcloud import WordCloud, STOPWORDS\nfrom surprise import Reader\nfrom surprise import Dataset\nfrom surprise.model_selection import cross_validate\nfrom surprise import NormalPredictor\nfrom surprise import KNNBasic\nfrom surprise import KNNWithMeans\nfrom surprise import KNNWithZScore\nfrom surprise import KNNBaseline\nfrom surprise import BaselineOnly\nfrom surprise import SVDpp\nfrom surprise import NMF\nfrom surprise import SlopeOne\nfrom surprise import CoClustering\nfrom surprise.accuracy import rmse\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline  import Pipeline\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder, PolynomialFeatures, Normalizer\nfrom sklearn.linear_model import LinearRegression, Lasso, Ridge\nfrom catboost import CatBoostRegressor\nfrom sklearn.metrics import mean_squared_error","metadata":{"execution":{"iopub.status.busy":"2022-05-01T18:20:03.272773Z","iopub.execute_input":"2022-05-01T18:20:03.273047Z","iopub.status.idle":"2022-05-01T18:20:05.822011Z","shell.execute_reply.started":"2022-05-01T18:20:03.273011Z","shell.execute_reply":"2022-05-01T18:20:05.821233Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import gc\ngc.collect()\n","metadata":{"execution":{"iopub.status.busy":"2022-05-01T18:20:05.823972Z","iopub.execute_input":"2022-05-01T18:20:05.824254Z","iopub.status.idle":"2022-05-01T18:20:05.972524Z","shell.execute_reply.started":"2022-05-01T18:20:05.824216Z","shell.execute_reply":"2022-05-01T18:20:05.971499Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Table of contents:\n1. Introduction\n2. Import libaries and datasets\n3. Exploratory Data Analysis\nSummary statistics\nVisualizing the dataframes\nVisualizing the null values for each dataframe\nVisualizing common users\nExploring movie genres\nExploring the movies dataframe\nWord cloud\nPublishing years\nBudget\n4. Prepocessing\n5. Modelling\nContent-Based Filtering Recommendation\nCollaborative-Based Filtering Reccomendation\n6. Evaluation\n7. Conclusion","metadata":{}},{"cell_type":"markdown","source":"## Loading data","metadata":{}},{"cell_type":"code","source":"#Pandas libraries used in the notebook.\ntrain = pd.read_csv('../input/edsa-movie-recommendation-2022/train.csv')\ntest_df = pd.read_csv('../input/edsa-movie-recommendation-2022/test.csv')\ndf_movies = pd.read_csv('../input/edsa-movie-recommendation-2022/movies.csv')\ndf_samp = pd.read_csv('../input/edsa-movie-recommendation-2022/sample_submission.csv')\ndf_imdb = pd.read_csv('../input/edsa-movie-recommendation-2022/imdb_data.csv')\ngenome_tags = pd.read_csv(\"../input/edsa-movie-recommendation-2022/genome_tags.csv\")\ngenome_score = pd.read_csv(\"../input/edsa-movie-recommendation-2022/genome_scores.csv\")\ndf_tags = pd.read_csv(\"../input/edsa-movie-recommendation-2022/tags.csv\")\ndf_links = pd.read_csv(\"../input/edsa-movie-recommendation-2022/links.csv\")","metadata":{"execution":{"iopub.status.busy":"2022-05-01T18:20:07.672406Z","iopub.execute_input":"2022-05-01T18:20:07.67299Z","iopub.status.idle":"2022-05-01T18:20:28.805092Z","shell.execute_reply.started":"2022-05-01T18:20:07.672949Z","shell.execute_reply":"2022-05-01T18:20:28.80429Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Exploratory Data Analysis","metadata":{}},{"cell_type":"markdown","source":"##### Checking the shapes of our Datasets","metadata":{}},{"cell_type":"code","source":"print('train data:',train.shape) \nprint('test data:',test_df.shape)\nprint('tags data:',df_tags.shape)\nprint(\"Movies data:\",df_movies.shape)\nprint('links data:',df_links.shape)\nprint('imdb data:',df_imdb.shape)\nprint('genome tags data:',genome_tags.shape)\nprint('genome scores data:',genome_score.shape)","metadata":{"execution":{"iopub.status.busy":"2022-05-01T18:20:28.806624Z","iopub.execute_input":"2022-05-01T18:20:28.806892Z","iopub.status.idle":"2022-05-01T18:20:28.816825Z","shell.execute_reply.started":"2022-05-01T18:20:28.806855Z","shell.execute_reply":"2022-05-01T18:20:28.81599Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Let us Visualize our dataframe","metadata":{}},{"cell_type":"markdown","source":"##### First, we would be calculating the sizes of our data, which would we compare using a Piechart and a Barplot","metadata":{}},{"cell_type":"code","source":"\"\"\"\" Creating a DataFrame which would outline the sizes of our data\n\"\"\"\n\nD_F = ['train', 'test_df', 'df_tags', 'df_imdb','links_df', 'df_movies', 'genome_tags', 'genome_score']\n\nsizes = [len(train), len(test_df), len(df_tags),\n         len(df_imdb), len(df_links), len(df_movies),\n         len(genome_tags), len(genome_score)]\ntotal_size_df = pd.DataFrame(list(zip(D_F, sizes)),\n                             columns=['dataframe', 'sizes'])\ntotal_size_df","metadata":{"execution":{"iopub.status.busy":"2022-05-01T18:12:49.091543Z","iopub.execute_input":"2022-05-01T18:12:49.093814Z","iopub.status.idle":"2022-05-01T18:12:49.111147Z","shell.execute_reply.started":"2022-05-01T18:12:49.093777Z","shell.execute_reply":"2022-05-01T18:12:49.110228Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now let us separate the sizes into categorie, with sizes above 100000 and those below 100000 into category **Other** ","metadata":{}},{"cell_type":"code","source":"total_size_df = total_size_df[total_size_df['sizes'] > 100000]\ntotal_size_df","metadata":{"execution":{"iopub.status.busy":"2022-05-01T18:12:49.112674Z","iopub.execute_input":"2022-05-01T18:12:49.112939Z","iopub.status.idle":"2022-05-01T18:12:49.127348Z","shell.execute_reply.started":"2022-05-01T18:12:49.112905Z","shell.execute_reply":"2022-05-01T18:12:49.126568Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_row = {'dataframe': 'other', 'sizes': 180530}\ntotal_size_df = total_size_df.append(new_row, ignore_index=True)\ntotal_size_df","metadata":{"execution":{"iopub.status.busy":"2022-05-01T18:12:49.129035Z","iopub.execute_input":"2022-05-01T18:12:49.129306Z","iopub.status.idle":"2022-05-01T18:12:49.142648Z","shell.execute_reply.started":"2022-05-01T18:12:49.129261Z","shell.execute_reply":"2022-05-01T18:12:49.14135Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The DataFrames that had a larger size than 100000 include the train_df; test_df; tags_df; genome_score and other. \n\nWhile the DataFrame's that had a size smaller than 100000 were combined into a DataFrame 'other', these DataFrames are imdb_df; links_df, movies_df; and genome_tags.","metadata":{}},{"cell_type":"code","source":"# explodeTuple = (0.05, 0.04, 0.05, 0.04, 0.6)\n# fig1, ax1 = plt.subplots(figsize=(14,7))\n# ax1.pie(total_size_df['sizes'].values,\n#         labels=total_size_df['dataframe'].values,\n#         startangle=90, autopct='%1.1f%%',\n#         explode=explodeTuple)\n# ax1.axis('equal')\n# plt.title('Distribution of overall Data Frames')\n# plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-01T18:12:49.146265Z","iopub.execute_input":"2022-05-01T18:12:49.146538Z","iopub.status.idle":"2022-05-01T18:12:49.150745Z","shell.execute_reply.started":"2022-05-01T18:12:49.146506Z","shell.execute_reply":"2022-05-01T18:12:49.149827Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len_list = [['train', len(train)], ['df_tags', len(df_tags)],\n            ['df_imdb', len(df_imdb)], ['df_links', len(df_links)],\n            ['df_movies', len(df_movies)],\n            ['genome_tags', len(genome_tags)],\n            ['genome_score', len(genome_score)]]\nlen_df = pd.DataFrame(len_list,\n                      columns=['Dataset', 'Size'])\nfig = px.bar(len_df, x=len_df['Dataset'],\n             y=len_df['Size'],\n             color=len_df['Dataset'],\n             title='Distribution of overall Data Frames')\nfig.update_layout(legend=dict(\n    orientation=\"h\",\n    yanchor=\"bottom\",\n    y=1.02,\n    xanchor=\"right\",\n    x=1\n))\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-01T18:12:49.152539Z","iopub.execute_input":"2022-05-01T18:12:49.152798Z","iopub.status.idle":"2022-05-01T18:12:49.952914Z","shell.execute_reply.started":"2022-05-01T18:12:49.152765Z","shell.execute_reply":"2022-05-01T18:12:49.952169Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Observation:\nIn the bar graph, there is a clear uneven distribution of the data sizes in each dataset\nThe genome_score DataFrame has the largest size, followed by the train_df.     \n\nThe difference in distribution sizes is clear, where the other DataFrame's bars aren't visually evident because of the large difference between the dimension","metadata":{}},{"cell_type":"markdown","source":"##### Let us Visualize Null Values present","metadata":{}},{"cell_type":"code","source":"# First we would obtain the total null values in each Data Frames columns\n\ntrain_count = pd.DataFrame(train.isnull().sum())\ntest_count = pd.DataFrame(test_df.isnull().sum())\ntags_count = pd.DataFrame(df_tags.isnull().sum())\nmovies_count = pd.DataFrame(df_movies.isnull().sum())\nlinks_count = pd.DataFrame(df_links.isnull().sum())\nimdb_count = pd.DataFrame(df_imdb.isnull().sum())\ngenomet_count = pd.DataFrame(genome_tags.isnull().sum())\ngenomes_count = pd.DataFrame(genome_score.isnull().sum())\n\n","metadata":{"execution":{"iopub.status.busy":"2022-05-01T18:12:49.954192Z","iopub.execute_input":"2022-05-01T18:12:49.954943Z","iopub.status.idle":"2022-05-01T18:12:50.244086Z","shell.execute_reply.started":"2022-05-01T18:12:49.954904Z","shell.execute_reply":"2022-05-01T18:12:50.243319Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Fromm the output above, we can see that only **df_tags** and **df_imdb** show null values.\n\nLet us visualize thise for better clarity","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=[7,5])\nplt.bar(tags_count.index,\n        tags_count.values.reshape(len(tags_count), ),\n        color='red')\nplt.xlabel('column_name')\nplt.ylabel('count')\nplt.title('Null value count in tags_df')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-01T18:12:50.245384Z","iopub.execute_input":"2022-05-01T18:12:50.245644Z","iopub.status.idle":"2022-05-01T18:12:50.454403Z","shell.execute_reply.started":"2022-05-01T18:12:50.245611Z","shell.execute_reply":"2022-05-01T18:12:50.453728Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=[7,5])\nplt.bar(imdb_count.index,\n        imdb_count.values.reshape(len(imdb_count), ),\n        color=['orange','blue','green','black','purple'])\nplt.xlabel('column_name')\nplt.ylabel('count')\nplt.title('Null value count in imdb_df')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-01T18:12:50.45555Z","iopub.execute_input":"2022-05-01T18:12:50.455794Z","iopub.status.idle":"2022-05-01T18:12:50.658936Z","shell.execute_reply.started":"2022-05-01T18:12:50.45576Z","shell.execute_reply":"2022-05-01T18:12:50.658224Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Now we will visualize common users","metadata":{}},{"cell_type":"markdown","source":"We would try to calculate the number of times a user rated a movie by first creating a dataframe that matches a count by userId","metadata":{}},{"cell_type":"code","source":"##Create DataFrame with count by userID\nuser_count = pd.DataFrame(train['userId'].value_counts()).reset_index()\nuser_count.rename(columns={'index':'userId','userId':'count'},\n                  inplace=True)\nuser_count.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-01T18:12:50.660223Z","iopub.execute_input":"2022-05-01T18:12:50.660585Z","iopub.status.idle":"2022-05-01T18:12:51.081678Z","shell.execute_reply.started":"2022-05-01T18:12:50.660546Z","shell.execute_reply":"2022-05-01T18:12:51.080876Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Next, we will group the users within a range, by doing so, we can differentiate between common users and new ones\n\n","metadata":{}},{"cell_type":"code","source":"## Grouping users by count within range\nfirst_group = user_count.loc[(user_count['count'] > 0) &  #between 1 - 50\n            (user_count['count'] < 50),\n            'userId'].value_counts().sum()\nsecond_group = user_count.loc[(user_count['count'] >= 50) &  #between 50 - 500\n            (user_count['count'] < 500),\n            'userId'].value_counts().sum()\nthird_group = user_count.loc[(user_count['count'] >= 500) & #between 500 - 1000\n            (user_count['count'] < 1000),\n            'userId'].value_counts().sum()\nfourth_group = user_count.loc[(user_count['count'] >= 1000) & #between 1000 - 1500\n            (user_count['count'] < 1500),\n            'userId'].value_counts().sum()\nfifth_group = user_count.loc[(user_count['count'] >= 1500),#from 1500 above\n            'userId'].value_counts().sum()","metadata":{"execution":{"iopub.status.busy":"2022-05-01T18:12:51.083067Z","iopub.execute_input":"2022-05-01T18:12:51.083435Z","iopub.status.idle":"2022-05-01T18:12:51.115042Z","shell.execute_reply.started":"2022-05-01T18:12:51.083393Z","shell.execute_reply":"2022-05-01T18:12:51.114365Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# To give us insight in the spread, we used figures to determine the spread.\ntrial_error = np.array([['first_group', first_group,\n                         'between 1 and 50'],\n                        ['second_group', second_group,\n                         'between 50 and 500'],\n                        ['third_group', third_group,\n                         'between 500 and 1000'],\n                        ['fourth_group', fourth_group,\n                         'between 1000 and 1500'],\n                        ['fifth_group', fifth_group,\n                         'greater than 1500']])\n\n\ntrial_error_df = pd.DataFrame({'group': trial_error[:, 0],\n                               'userId_grouping': trial_error[:, 1],\n                               'explanation': trial_error[:, 2]})\nfig = px.bar(trial_error_df,\n             x=trial_error_df[\"group\"],\n             y=trial_error_df[\"userId_grouping\"],\n             color=trial_error_df[\"group\"],\n             title='Grouped Rating Distribution')\n\nfig.update_layout(legend=dict(\n    orientation=\"h\",\n    yanchor=\"bottom\",\n    y=1.02,\n    xanchor=\"right\",\n    x=1\n))\nfig.show()\ntrial_error_df","metadata":{"execution":{"iopub.status.busy":"2022-05-01T18:12:51.116465Z","iopub.execute_input":"2022-05-01T18:12:51.11673Z","iopub.status.idle":"2022-05-01T18:12:51.20962Z","shell.execute_reply.started":"2022-05-01T18:12:51.116693Z","shell.execute_reply":"2022-05-01T18:12:51.208672Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Let us explain what is happening here:**\n\nThe user Id's are grouped by the rating counts in a grouping range illustrated in the DataFrame above.\n\nIn the Grouped Rating Distribution bar graph, it is visually displayed that there is unequal distribution. The distribution is skewed to the left, with the majority of the user ids in the rating count range between 1 and 50. At the same time, the last group has only a value count of 61, which is a significant difference from group one with a value count of 110 010.","metadata":{}},{"cell_type":"code","source":"def user_ratings_count(df, n):\n    plt.figure(figsize=(14,7))\n    data = df['userId'].value_counts().head(n)\n    ax = sns.barplot(x = data.index, y = data, order= data.index, palette='CMRmap', edgecolor=\"black\")\n    for p in ax.patches:\n        ax.text(p.get_x() + p.get_width()/2., p.get_height(), '%d' % int(p.get_height()), fontsize=11, ha='center', va='bottom')\n    plt.title(f'Top {n} Users by Number of Ratings', fontsize=14)\n    plt.xlabel('User ID')\n    plt.ylabel('Number of Ratings')\n    plt.show()\n\nuser_ratings_count(train,10)","metadata":{"execution":{"iopub.status.busy":"2022-05-01T18:12:51.211205Z","iopub.execute_input":"2022-05-01T18:12:51.211503Z","iopub.status.idle":"2022-05-01T18:12:51.909896Z","shell.execute_reply.started":"2022-05-01T18:12:51.211464Z","shell.execute_reply":"2022-05-01T18:12:51.909194Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that UserID **72315** shows some form of uneven distribution which is as a result of having too many ratings as compared to others, hence deemed to be an outlier","metadata":{}},{"cell_type":"markdown","source":"#### Now we will begin to explore the *Movie* Data for insights","metadata":{}},{"cell_type":"code","source":"#Let us merge the train data withh the movie data\nmovies = pd.merge(train, df_movies,on='movieId',how='inner')\nmovies.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-01T18:12:51.911136Z","iopub.execute_input":"2022-05-01T18:12:51.911607Z","iopub.status.idle":"2022-05-01T18:12:53.945911Z","shell.execute_reply.started":"2022-05-01T18:12:51.911566Z","shell.execute_reply":"2022-05-01T18:12:53.945183Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# We will add an additional data i.e df_imdb data\ncomplete_movie = pd.merge(movies,df_imdb,on='movieId',how='inner')\ncomplete_movie.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-01T18:12:53.947338Z","iopub.execute_input":"2022-05-01T18:12:53.947608Z","iopub.status.idle":"2022-05-01T18:12:56.034901Z","shell.execute_reply.started":"2022-05-01T18:12:53.947573Z","shell.execute_reply":"2022-05-01T18:12:56.034191Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### We will convert our timestamp and carry out some visualization, but for now let us continue","metadata":{}},{"cell_type":"code","source":"#Get the top ratings of movies\ndef top_rating_plot(df,column, n):\n    plt.figure(figsize=(14,7))\n    data = df[str(column)].value_counts().head(n)\n    ax = sns.barplot(x = data.index, y = data, order= data.index, palette='CMRmap', edgecolor=\"black\")\n    for p in ax.patches:\n        ax.text(p.get_x() + p.get_width()/2., p.get_height(), '%d' % int(p.get_height()), fontsize=11, ha='center', va='bottom')\n    plt.title(f'Top {n} {column.title()} by Number of Ratings', fontsize=14)\n    plt.xlabel(column.title())\n    plt.ylabel('Number of Ratings')\n    plt.xticks(rotation=90)\n    plt.show()\n\ntop_rating_plot(movies,'title',15)","metadata":{"execution":{"iopub.status.busy":"2022-05-01T18:12:56.036345Z","iopub.execute_input":"2022-05-01T18:12:56.036598Z","iopub.status.idle":"2022-05-01T18:12:57.381826Z","shell.execute_reply.started":"2022-05-01T18:12:56.036564Z","shell.execute_reply":"2022-05-01T18:12:57.381055Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the observation in the plot, what does this tell us:\n\n  * All the movies in the top 15 by Number of Ratings were released in the 90's with only one  i.e \"Lord of the Rings\" released in the year 2001","metadata":{}},{"cell_type":"code","source":"gc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-05-01T18:12:57.382826Z","iopub.execute_input":"2022-05-01T18:12:57.384474Z","iopub.status.idle":"2022-05-01T18:12:57.544772Z","shell.execute_reply.started":"2022-05-01T18:12:57.38443Z","shell.execute_reply":"2022-05-01T18:12:57.543988Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Exploring Movie Genre","metadata":{}},{"cell_type":"code","source":"#Get the categories of Movie Genre\nmovie_genres = pd.DataFrame(df_movies['genres'].str.split(\"|\").tolist(),\n                      index=df_movies['movieId']).stack()\nmovie_genres = movie_genres.reset_index([0, 'movieId'])\nmovie_genres.columns = ['movieId', 'Genre']\nmovie_genres.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-01T18:12:57.546283Z","iopub.execute_input":"2022-05-01T18:12:57.547178Z","iopub.status.idle":"2022-05-01T18:12:57.663017Z","shell.execute_reply.started":"2022-05-01T18:12:57.547135Z","shell.execute_reply":"2022-05-01T18:12:57.662178Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(14, 7))\nsns.countplot(x='Genre',\n              data=movie_genres,\n              palette='CMRmap',\n              order=movie_genres['Genre'].\n              value_counts().index)\nplt.xticks(rotation=90)\nplt.xlabel('Genre', size=20)\nplt.ylabel('Count', size=20)\nplt.title('Distribution of Movie Genres', size=25)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-01T18:12:57.666652Z","iopub.execute_input":"2022-05-01T18:12:57.666856Z","iopub.status.idle":"2022-05-01T18:12:58.10329Z","shell.execute_reply.started":"2022-05-01T18:12:57.666832Z","shell.execute_reply":"2022-05-01T18:12:58.102509Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Mere looking at the graph, we can tell that **Drama, Comedy,Thriller and Romance** stand out as the popular movie genres ","metadata":{}},{"cell_type":"markdown","source":"Several factors can be attributed to why we these genres stand out, viz:\n  * \n  *\n  *\n  ","metadata":{}},{"cell_type":"markdown","source":"#### Let us get an interesting wordcloud to showcase movie titles and the count of ratings","metadata":{}},{"cell_type":"code","source":"# Wordcloud of movie titles\nmovie = df_movies['title'] = df_movies['title'].astype('str')\nwordcloud = ' '.join(movie)\ntitle_wordcloud = WordCloud(stopwords = STOPWORDS,\n                            background_color = 'Black',\n                            height = 1200,\n                            width = 900).generate(wordcloud)\nplt.figure(figsize = (14,7), facecolor=None)\nplt.imshow(title_wordcloud)\nplt.axis('off')\nplt.title('Distribution of words from movie titles')\nplt.tight_layout(pad=0)\nplt.show()\n\ntop_rating_plot(movies,'rating',10)","metadata":{"execution":{"iopub.status.busy":"2022-05-01T18:12:58.104894Z","iopub.execute_input":"2022-05-01T18:12:58.105151Z","iopub.status.idle":"2022-05-01T18:13:02.719681Z","shell.execute_reply.started":"2022-05-01T18:12:58.105114Z","shell.execute_reply":"2022-05-01T18:13:02.718957Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"   * The most common rating we have is 4, followed closely by 3 and 5.\n\n   * The least common seen here is 0.5","metadata":{}},{"cell_type":"markdown","source":"#### Let us consider the Timestamps to get some insights ","metadata":{}},{"cell_type":"markdown","source":"##### Extracting datetime","metadata":{}},{"cell_type":"code","source":"import datetime","metadata":{"execution":{"iopub.status.busy":"2022-05-01T18:13:02.720821Z","iopub.execute_input":"2022-05-01T18:13:02.721596Z","iopub.status.idle":"2022-05-01T18:13:02.725623Z","shell.execute_reply.started":"2022-05-01T18:13:02.721552Z","shell.execute_reply":"2022-05-01T18:13:02.724922Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#From movie data, convert timestamp to datetime\nmovies['time_dt'] = movies['timestamp'].apply(lambda x: datetime.datetime.fromtimestamp(x))","metadata":{"execution":{"iopub.status.busy":"2022-05-01T18:13:02.727046Z","iopub.execute_input":"2022-05-01T18:13:02.727622Z","iopub.status.idle":"2022-05-01T18:13:13.339203Z","shell.execute_reply.started":"2022-05-01T18:13:02.727585Z","shell.execute_reply":"2022-05-01T18:13:13.33837Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"movies.head(3)","metadata":{"execution":{"iopub.status.busy":"2022-05-01T18:13:13.340468Z","iopub.execute_input":"2022-05-01T18:13:13.342488Z","iopub.status.idle":"2022-05-01T18:13:13.356019Z","shell.execute_reply.started":"2022-05-01T18:13:13.342451Z","shell.execute_reply":"2022-05-01T18:13:13.355242Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Let's also show the number and average value of ratings variation in time.","metadata":{}},{"cell_type":"code","source":"movies['year'] = movies['time_dt'].dt.year\nmovies['month'] = movies['time_dt'].dt.month\nmovies['day'] = movies['time_dt'].dt.day\nmovies['dayofweek'] = movies['time_dt'].dt.dayofweek","metadata":{"execution":{"iopub.status.busy":"2022-05-01T18:13:13.357497Z","iopub.execute_input":"2022-05-01T18:13:13.357933Z","iopub.status.idle":"2022-05-01T18:13:17.46863Z","shell.execute_reply.started":"2022-05-01T18:13:13.357889Z","shell.execute_reply":"2022-05-01T18:13:17.467874Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"movies.head(3)","metadata":{"execution":{"iopub.status.busy":"2022-05-01T18:13:17.469978Z","iopub.execute_input":"2022-05-01T18:13:17.47024Z","iopub.status.idle":"2022-05-01T18:13:17.484608Z","shell.execute_reply.started":"2022-05-01T18:13:17.470205Z","shell.execute_reply":"2022-05-01T18:13:17.48365Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dt = movies.groupby(['year'])['rating'].count().reset_index()\nfig, (ax) = plt.subplots(ncols=1, figsize=(12,6))\nplt.plot(dt['year'],dt['rating']); plt.xlabel('Year'); plt.ylabel('Number of votes'); plt.title('Number of votes per year')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-01T18:13:17.485854Z","iopub.execute_input":"2022-05-01T18:13:17.486807Z","iopub.status.idle":"2022-05-01T18:13:17.919981Z","shell.execute_reply.started":"2022-05-01T18:13:17.486768Z","shell.execute_reply":"2022-05-01T18:13:17.919226Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the line plot, we can see that the number of votes (ratings) peaked between **2015 - 2018**\n\nCan be attributed to several factors like;\n   * Quality of movies at the time\n   * Genre of movie\n   * //////\n   etc","metadata":{}},{"cell_type":"code","source":"dt = movies.groupby(['year'])['rating'].mean().reset_index()\nfig, (ax) = plt.subplots(ncols=1, figsize=(12,6))\nplt.plot(dt['year'],dt['rating']); plt.xlabel('Year'); plt.ylabel('Average ratings'); plt.title('Average ratings per year')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-01T18:13:17.921246Z","iopub.execute_input":"2022-05-01T18:13:17.921729Z","iopub.status.idle":"2022-05-01T18:13:18.314883Z","shell.execute_reply.started":"2022-05-01T18:13:17.921689Z","shell.execute_reply":"2022-05-01T18:13:18.314173Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Preprocessing","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Modeling","metadata":{}},{"cell_type":"markdown","source":"We would be considering 2 types here:\n  * Content-Based Filtering Recommendation\n  * Collaborative filtering ","metadata":{}},{"cell_type":"markdown","source":"### Content-Based Filtering Recommendation\n\nThe Content-Based Recommendation system computes similarity between movies based on movie genres using the selected movie as a baseline.","metadata":{}},{"cell_type":"code","source":"## Cosine-similarity\n\nfrom sklearn.metrics.pairwise import linear_kernel\n# Break up the big genre string into a string array\ngenre = df_movies['genres'].str.split('|')\ngenre = df_movies['genres'].fillna(\"\").astype('str')\nsample_genre = genre.head(20000)   # remove sample on EC2 to acess the whole dataset\n\n#create a Tf_idf vectorizer\ntf = TfidfVectorizer(analyzer = 'word',\n                     ngram_range = (1, 3),\n                     min_df = 0,\n                     stop_words = 'english')\n\nvecto = tf.fit_transform(sample_genre)     # replace sample_genre with genre\n# cosine similarities\ncosine_sim = linear_kernel(vecto, vecto)\n","metadata":{"execution":{"iopub.status.busy":"2022-05-01T18:13:18.316114Z","iopub.execute_input":"2022-05-01T18:13:18.317488Z","iopub.status.idle":"2022-05-01T18:13:24.989508Z","shell.execute_reply.started":"2022-05-01T18:13:18.317444Z","shell.execute_reply":"2022-05-01T18:13:24.988643Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The TF-IDF vectorizer was used to find the relationship in terms of relative importance of the movie. The systems make use of Cosine Similarity to compute numeric quantities that highlights the similarities between the movies (one as a baseline).","metadata":{}},{"cell_type":"code","source":"\n# The movie_recommendation function\ndef movie_recommendations(movie):\n    # set the movie title as the new index\n    movie_index=pd.Series(df_movies.index,\n                          index = df_movies['title'])\n    # generate  similarities between the movie title and movie index based on genre\n    similarities = list(enumerate(cosine_sim[movie_index[movie]]))\n    similarities = sorted(similarities, key = lambda x: x[1],\n                          reverse = True)[1:11]\n    movie_recommendation = df_movies['title'].iloc[[i[0] for i in similarities]]\n    return movie_recommendation","metadata":{"execution":{"iopub.status.busy":"2022-05-01T18:13:24.991019Z","iopub.execute_input":"2022-05-01T18:13:24.991283Z","iopub.status.idle":"2022-05-01T18:13:24.997189Z","shell.execute_reply.started":"2022-05-01T18:13:24.991245Z","shell.execute_reply":"2022-05-01T18:13:24.996429Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"movie_recommendations('Into the West (2005)')","metadata":{"execution":{"iopub.status.busy":"2022-05-01T18:13:24.998575Z","iopub.execute_input":"2022-05-01T18:13:24.999024Z","iopub.status.idle":"2022-05-01T18:13:25.036103Z","shell.execute_reply.started":"2022-05-01T18:13:24.998985Z","shell.execute_reply":"2022-05-01T18:13:25.035361Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"A function was assembled to estimate top 10 movies that are similar to the base movie using genre as a basis. The function was passed an input (‘Into the West (2005)’), a baseline and generated top 10 similar movies using cosine similarity score computed above.","metadata":{}},{"cell_type":"markdown","source":"### Collaborative-Based Filtering Reccomendation\nCollaborative filtering addresses some of the limitations of content-based filtering; collaborative filtering uses similarities between users and items simultaneously to provide recommendations.\nCollaborative filtering models can recommend an item to **user A** based on the interests of a similar **user B**. \n\nFurthermore, the embeddings can be learned automatically, without relying on hand-engineering of features.","metadata":{}},{"cell_type":"markdown","source":"Taking the first 1  million dataset to train on ","metadata":{}},{"cell_type":"markdown","source":"### Splitting Data into Train and Test Set","metadata":{}},{"cell_type":"code","source":"#Train and Test set\ntrain.drop('timestamp', axis=1)\ntrain_subset = train[:1000000]\nreader = Reader(rating_scale=(train_subset['rating'].min(), train_subset['rating'].max()))\ndata = Dataset.load_from_df(train_subset[['userId', 'movieId', 'rating']], reader)\ntrainset, testset = train_test_split(data, test_size=.25, random_state=42)\n","metadata":{"execution":{"iopub.status.busy":"2022-05-01T18:20:52.486382Z","iopub.execute_input":"2022-05-01T18:20:52.48714Z","iopub.status.idle":"2022-05-01T18:20:56.304276Z","shell.execute_reply.started":"2022-05-01T18:20:52.4871Z","shell.execute_reply":"2022-05-01T18:20:56.30345Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-05-01T18:21:00.162189Z","iopub.execute_input":"2022-05-01T18:21:00.162845Z","iopub.status.idle":"2022-05-01T18:21:00.507452Z","shell.execute_reply.started":"2022-05-01T18:21:00.162802Z","shell.execute_reply":"2022-05-01T18:21:00.506517Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Now we will consider several Algorithms to test with","metadata":{}},{"cell_type":"markdown","source":"#### 1. SlopeOne Algorithm\n\nSlope One is a family of algorithms used for collaborative filtering. \nIt is the simplest form of non-trivial item-based collaborative filtering based on ratings\n\nTheir simplicity makes it especially easy to implement them efficiently while their accuracy is often on par with more complicated and computationally expensive algorithms","metadata":{}},{"cell_type":"code","source":"slo_model = SlopeOne()\nslo_model.fit(trainset)\nslo_predictions = slo_model.test(testset)\nslo_rmse=accuracy.rmse(slo_predictions)","metadata":{"execution":{"iopub.status.busy":"2022-05-01T18:13:28.703853Z","iopub.execute_input":"2022-05-01T18:13:28.704364Z","iopub.status.idle":"2022-05-01T18:14:01.226982Z","shell.execute_reply.started":"2022-05-01T18:13:28.704257Z","shell.execute_reply":"2022-05-01T18:14:01.226155Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-05-01T18:18:07.97834Z","iopub.execute_input":"2022-05-01T18:18:07.979018Z","iopub.status.idle":"2022-05-01T18:18:08.416194Z","shell.execute_reply.started":"2022-05-01T18:18:07.978975Z","shell.execute_reply":"2022-05-01T18:18:08.415405Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Non-Negative Matrix Factorization Algorithm\nNon-negative matrix factorization, also called non-negative matrix approximation is a group of algorithms in multivariate analysis and linear algebra where a matrix V is factorized into two matrices W and H, with the property that all three matrices have no negative elements.","metadata":{}},{"cell_type":"code","source":"nmf_model = NMF(n_epochs=10, n_factors=200, random_state=42,verbose=True)\nnmf_model.fit(trainset)\nnmf_predictions =nmf_model.test(testset)\nnmf_rmse = accuracy.rmse(nmf_predictions)","metadata":{"execution":{"iopub.status.busy":"2022-05-01T18:21:08.537999Z","iopub.execute_input":"2022-05-01T18:21:08.538299Z","iopub.status.idle":"2022-05-01T18:23:21.334375Z","shell.execute_reply.started":"2022-05-01T18:21:08.538264Z","shell.execute_reply":"2022-05-01T18:23:21.333408Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-05-01T18:23:21.33639Z","iopub.execute_input":"2022-05-01T18:23:21.336879Z","iopub.status.idle":"2022-05-01T18:23:21.756558Z","shell.execute_reply.started":"2022-05-01T18:23:21.336834Z","shell.execute_reply":"2022-05-01T18:23:21.755781Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### CoClustering Algorithm\nBiclustering, block clustering , co-clustering, or two-mode clustering is a data mining technique which allows simultaneous clustering of the rows and columns of a matrix. ","metadata":{}},{"cell_type":"code","source":"cc_model = CoClustering(n_epochs=10,random_state=42)\ncc_model.fit(trainset)\ncc_predictions = cc_model.test(testset)\ncc_rmse=accuracy.rmse(cc_predictions)","metadata":{"execution":{"iopub.status.busy":"2022-05-01T18:23:21.758018Z","iopub.execute_input":"2022-05-01T18:23:21.758498Z","iopub.status.idle":"2022-05-01T18:23:47.463287Z","shell.execute_reply.started":"2022-05-01T18:23:21.758458Z","shell.execute_reply":"2022-05-01T18:23:47.461636Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-05-01T18:23:47.465206Z","iopub.execute_input":"2022-05-01T18:23:47.465486Z","iopub.status.idle":"2022-05-01T18:23:47.953159Z","shell.execute_reply.started":"2022-05-01T18:23:47.465444Z","shell.execute_reply":"2022-05-01T18:23:47.952403Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Singular Value Decomposition (SVD)\nThe singular value decomposition (SVD) provides another way to factorize a matrix, into singular vectors and singular values. The SVD allows us to discover some of the same kind of information as the eigendecomposition.The SVD is used widely both in the calculation of other matrix operations, such as matrix inverse, but also as a data reduction method in machine learning. SVD can also be used in least squares linear regression, image compression, and denoising data.","metadata":{}},{"cell_type":"code","source":"svd_model = SVD(n_epochs=60,n_factors=400,init_std_dev=0.005,random_state=42,verbose=True)\nsvd_model.fit(trainset)\nsvd_predictions = svd_model.test(testset)\nsvd_rmse = accuracy.rmse(svd_predictions)","metadata":{"execution":{"iopub.status.busy":"2022-05-01T18:38:07.007065Z","iopub.execute_input":"2022-05-01T18:38:07.007361Z","iopub.status.idle":"2022-05-01T18:49:26.760254Z","shell.execute_reply.started":"2022-05-01T18:38:07.007322Z","shell.execute_reply":"2022-05-01T18:49:26.759474Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-05-01T18:37:51.897145Z","iopub.execute_input":"2022-05-01T18:37:51.897518Z","iopub.status.idle":"2022-05-01T18:37:52.594159Z","shell.execute_reply.started":"2022-05-01T18:37:51.89748Z","shell.execute_reply":"2022-05-01T18:37:52.59338Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Singular Value Decomposition plus-plus (SVDpp)\nThe SVD++ algorithm, an extension of SVD taking into account implicit ratings.","metadata":{}},{"cell_type":"code","source":"svdpp_model = SVDpp(n_epochs=10,n_factors=400,init_std_dev=0.001,random_state=42, verbose=True)\nsvdpp_model.fit(trainset)\nsvdpp_predictions = svdpp_model.test(testset)\nsvdpp_rmse = accuracy.rmse(svdpp_predictions)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### BaselineOnly algorithm¶\nAlgorithm predicting the baseline estimate for given user and item.","metadata":{}},{"cell_type":"code","source":"bsl_options = {'method': 'sgd','n_epochs': 10}\nblo_model = BaselineOnly(bsl_options=bsl_options,verbose=True)\nblo_model.fit(trainset)\nblo_predictions = blo_model.test(testset)\n# Calculate RMSE\nblo_rmse = accuracy.rmse(blo_predictions)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-04-30T19:07:54.6362Z","iopub.execute_input":"2022-04-30T19:07:54.636543Z","iopub.status.idle":"2022-04-30T19:07:57.531473Z","shell.execute_reply.started":"2022-04-30T19:07:54.636508Z","shell.execute_reply":"2022-04-30T19:07:57.530468Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Performance/Evaluation","metadata":{}},{"cell_type":"markdown","source":"We built and tested six different collaborative filtering models and compared their performance using a statistical measure known as the root mean squared error (RMSE), which determines the average squared difference between the estimated values and the actual value. A low RMSE value indicates high model accuracy.","metadata":{}},{"cell_type":"markdown","source":"#### Let us see which of the algorithms give us the lowest (better RMSE) score","metadata":{}},{"cell_type":"code","source":"rmse_scores =[nmf_rmse,slo_rmse,cc_rmse,svd_rmse,svdpp_rmse,blo_rmse]\nmodels =['NMF','SlopeOne','CoClustering','SVD','SVD++','BaselineOnly']\n\naccuracy_data = pd.DataFrame({'model':models,'RMSE':rmse_scores})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"accuracy_data.sort_values(by='RMSE')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Better Visualized in a graph","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(14,7))\nsns.barplot(data=accuracy_data.sort_values(by='RMSE'), x='model', y='RMSE', palette=\"CMRmap\", edgecolor=\"black\", ax=ax)\nax.set_xlabel(\"Model\")\nax.set_ylabel('RMSE Score')\nax.set_yticklabels(['{:,}'.format(int(x)) for x in ax.get_yticks().tolist()])\nfor p in ax.patches:\n    ax.text(p.get_x() + p.get_width()/2, p.get_y() + p.get_height(), round(p.get_height(),3), fontsize=12, ha=\"center\", va='bottom')\nplt.title('Model Accuracy By RMSE Score', fontsize=14)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Saving and Exporting model rating as a csv File","metadata":{}},{"cell_type":"code","source":"#read the test data\ntest_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-01T18:53:55.76493Z","iopub.execute_input":"2022-05-01T18:53:55.765204Z","iopub.status.idle":"2022-05-01T18:53:55.779754Z","shell.execute_reply.started":"2022-05-01T18:53:55.765173Z","shell.execute_reply":"2022-05-01T18:53:55.778618Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#writing  a function that does the whole process \npredictions = []\nfor i, row in test_df.iterrows():\n    x = (algo.predict(row.userId, row.movieId))\n    pred = x[3]\n    predictions.append(pred)\ntest_df['newID'] = test_df['userId'].map(str) +'_'+ test_df['movieId'].map(str)\nresults = pd.DataFrame({\"Id\":test_df['newID'],\"rating\": predictions})\n# Saves as CSV \nresults.to_csv(\"Team_4.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2022-05-01T23:02:52.751921Z","iopub.execute_input":"2022-05-01T23:02:52.752289Z","iopub.status.idle":"2022-05-01T23:10:15.098837Z","shell.execute_reply.started":"2022-05-01T23:02:52.752255Z","shell.execute_reply":"2022-05-01T23:10:15.09781Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#View Result\nresults","metadata":{"execution":{"iopub.status.busy":"2022-05-01T23:13:52.543041Z","iopub.execute_input":"2022-05-01T23:13:52.543449Z","iopub.status.idle":"2022-05-01T23:13:52.559066Z","shell.execute_reply.started":"2022-05-01T23:13:52.543411Z","shell.execute_reply":"2022-05-01T23:13:52.558293Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}